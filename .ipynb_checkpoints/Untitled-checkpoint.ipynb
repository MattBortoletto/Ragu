{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import SGD, Adam, Adadelta, RMSprop\n",
    "from keras.layers import Conv1D, Dense, MaxPooling1D, Flatten, Dropout\n",
    "from keras.layers import Embedding, GlobalAveragePooling1D, LSTM, SimpleRNN, GRU\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 10000 entries\n",
      "Finished 20000 entries\n",
      "Finished 30000 entries\n",
      "Finished 40000 entries\n",
      "Finished 50000 entries\n",
      "Finished 60000 entries\n",
      "Finished 70000 entries\n"
     ]
    }
   ],
   "source": [
    "intr_file = '../data/hg19_intr_clean.fa'\n",
    "depl_file = '../data/hg19_depl_clean.fa'\n",
    "\n",
    "e = 0\n",
    "intr_seqs = []\n",
    "depl_seqs = []\n",
    "for intr, depl in zip(SeqIO.parse(intr_file, 'fasta'), SeqIO.parse(depl_file, 'fasta')):\n",
    "    \n",
    "    step = 200; jump = 1; a = 0; b = step; n_jumps = 5\n",
    "    for j in range(n_jumps):\n",
    "        s_intr = str(intr.seq)[a:b]\n",
    "        s_depl = str(depl.seq)[a:b]\n",
    "        intr_seqs.append(s_intr)\n",
    "        depl_seqs.append(s_depl)\n",
    "        a = a + jump\n",
    "        b = a + step\n",
    "    \n",
    "    e = e + 1\n",
    "    if e%10000 == 0:\n",
    "        print('Finished ' + str(e) + ' entries')\n",
    "        \n",
    "def getKmers(sequence, size):\n",
    "    return [sequence[x:x+size].upper() for x in range(len(sequence) - size + 1)]\n",
    "    \n",
    "kmer = 10\n",
    "intr_texts = [(getKmers(i, kmer)) for i in intr_seqs]\n",
    "depl_texts = [(getKmers(i, kmer)) for i in depl_seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-58f46209d810>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mencoded_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmerge_texts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-58f46209d810>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mencoded_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmerge_texts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "merge_texts = intr_texts + depl_texts\n",
    "labels = list(np.ones(len(intr_texts))) + list(np.zeros(len(depl_texts)))\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(merge_texts)\n",
    "encoded_docs = tokenizer.texts_to_sequences(merge_texts)\n",
    "max_length = max([len(s.split()) for s in merge_texts])\n",
    "X = pad_sequences(encoded_docs, maxlen = max_length, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = X_train[:200000]\n",
    "X_test1 = X_test[:5000]\n",
    "y_train1 = y_train[:200000]\n",
    "y_test1 = y_test[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "from IPython.display import clear_output\n",
    "\n",
    "prts=[10] \n",
    "\n",
    "for prt in prts: \n",
    "    diffs=np.zeros((200-prt+1))\n",
    "    for i in range(200-prt+1): \n",
    "        print(i , \"/\", X_test1.shape[1])\n",
    "        X_eva_test=np.array(X_test1)\n",
    "        \n",
    "        reverse_word_map = pd.Series(dict(map(reversed, tokenizer.word_index.items()))) # inverted tokenizer\n",
    "        X_tmp0=np.array(reverse_word_map[X_eva_test.reshape(X_eva_test.shape[0]*X_eva_test.shape[1])]).reshape(\n",
    "            X_eva_test.shape[0],X_eva_test.shape[1]) # coverti tutto in kmers, serve aggiustare la shape\n",
    "\n",
    "        X_tmp0=X_tmp0[:,range(0,X_tmp0.shape[1],kmer)]  # becca solo kmer che non overlap\n",
    "        X_tmp0=np.array(list(np.sum(np.sum(X_tmp0,axis=1), axis=0))).reshape(X_tmp0.shape[0], 200) \n",
    "        #qua otteniamo una matrice di Nx200 nucleotidi\n",
    "        \n",
    "        range_helper=[k for k in range(0,X_tmp0.shape[0])]\n",
    "        np.random.shuffle(range_helper) # shuffle\n",
    "        X_tmp0[:,i:i+prt]=X_tmp0[range_helper,i:i+prt] # applica shuffle\n",
    "        X_tmp0=[''.join(X_tmp0[j,:]) for j in range(X_tmp0.shape[0])] # recupera sequenza nucleotidi\n",
    "        X_tmp0=[' '.join(getKmers(seq,kmer)) for seq in X_tmp0] # recupera sequenza kmers\n",
    "    \n",
    "        #tokenizer\n",
    "        X_tmp0=tokenizer.texts_to_sequences(X_tmp0)\n",
    "        X_tmp0= pad_sequences(X_tmp0, maxlen = max_length, padding = 'post')\n",
    "        print(X_tmp0.shape)\n",
    "        \n",
    "        c=model.evaluate(X_tmp0,np.array(y_test1))[1]\n",
    "        diffs[i]=(1-c) # 1 - accuracy = DeltaA\n",
    "    \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        print(\" iteration \", i, \" : perturbated res = \", c , \"saved value = \" , diffs[i])\n",
    "        print(\"Debug print\")\n",
    "        print(X_eva_test[0][10:20]) \n",
    "        print(X_tmp0[0][10:20])\n",
    "        print(X_tmp0.shape)\n",
    "\n",
    "    np.savetxt(\"important_locs_nnn\"+str(prt)+\"_.txt\", diffs)    \n",
    "    plt.scatter(range(0,len(diffs)), diffs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from IPython.display import clear_output\n",
    "\n",
    "intr_texts =np.array(intr_texts)\n",
    "depl_texts =np.array(depl_texts) \n",
    "\n",
    "freqs_intr=[]\n",
    "\n",
    "datao = np.loadtxt(\"important_locs.txt\")\n",
    "datao=np.convolve(datao,[1,2,3,4,5,6,7,8,9,10,9,8,7,6,5,4,3,2,1], \"same\")\n",
    "\n",
    "# second derivative for C prescription\n",
    "datao2=(datao[2:]-2*datao[1:-1]+datao[:-2])/sum(abs(datao))\n",
    "datao2=np.array(np.convolve(datao2,[1,2,3,4,5,6,5,4,3,2,1], \"same\"))\n",
    "\n",
    "datao=datao-min(datao)\n",
    "\n",
    "#### C ####\n",
    "datao2[datao2>0]=0\n",
    "datao2[datao2<0]=1\n",
    "datao2=np.concatenate((np.array([0.0,0.0]), datao2.astype(int)))\n",
    "###########\n",
    "\n",
    "plt.plot(range(len(datao)-2), (datao[2:]), label=\"all\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### A ####\n",
    "mydicti=Counter()\n",
    "for i in range(intr_texts.shape[1]):\n",
    "    counts = Counter(intr_texts[:,i])\n",
    "    for k in counts.keys():\n",
    "        counts[k]*=datao[i]\n",
    "    mydicti+=counts\n",
    "\n",
    "#mydictd=Counter()    \n",
    "for i in range(depl_texts.shape[1]):\n",
    "    clear_output(wait=True)\n",
    "    print(i)\n",
    "    counts = Counter(depl_texts[:,i])\n",
    "    for k in counts.keys():\n",
    "        counts[k]*=datao[i]\n",
    "    mydicti+=counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(mydicti.most_common(50))[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### C ####\n",
    "datao1=datao*datao2\n",
    "datao1=datao1/sum(datao1)\n",
    "\n",
    "mydicti=Counter()\n",
    "for i in range(intr_texts.shape[1]):\n",
    "    counts = Counter(intr_texts[:,i])\n",
    "    for k in counts.keys():\n",
    "        counts[k]*=datao1[i]\n",
    "    mydicti+=counts\n",
    "\n",
    "#mydictd=Counter()   \n",
    "for i in range(depl_texts.shape[1]):\n",
    "    clear_output(wait=True)\n",
    "    print(i)\n",
    "    counts = Counter(depl_texts[:,i])\n",
    "    for k in counts.keys():\n",
    "        counts[k]*=datao1[i]\n",
    "    mydicti+=counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(mydicti.most_common(50))[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### B ####\n",
    "mydicti=Counter()\n",
    "for i in range(intr_texts.shape[1]):\n",
    "    counts = Counter(intr_texts[:,i])\n",
    "    for k in counts.keys():\n",
    "        counts[k]*=1\n",
    "    mydicti+=counts\n",
    "\n",
    "#mydictd=Counter()\n",
    "for i in range(depl_texts.shape[1]):\n",
    "    clear_output(wait=True)\n",
    "    print(i)\n",
    "    counts = Counter(depl_texts[:,i])\n",
    "    for k in counts.keys():\n",
    "        counts[k]*=1\n",
    "    mydicti+=counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(mydicti.most_common(50))[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### D #### \n",
    "maxpos=[]\n",
    "maxdict=Counter() \n",
    "\n",
    "for i in range(len(datao)-1):\n",
    "    if(datao[i]>datao[i-1] and datao[i]>datao[i+1]):\n",
    "        maxpos.append(i)\n",
    "\n",
    "for i in maxpos:\n",
    "    counts = Counter(intr_texts[:,i])\n",
    "    for k in counts.keys():\n",
    "        counts[k]*=1\n",
    "    maxdict+=counts\n",
    "     \n",
    "for i in maxpos:\n",
    "    clear_output(wait=True)\n",
    "    print(i)\n",
    "    counts = Counter(depl_texts[:,i])\n",
    "    for k in counts.keys():\n",
    "        counts[k]*=1\n",
    "    maxdict+=counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(maxdict.most_common(50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
