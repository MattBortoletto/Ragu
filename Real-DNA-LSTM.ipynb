{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM on real DNA sequences\n",
    "\n",
    "# Leggi qui\n",
    "Nella parte prima di START HERE ho commentato le linee che non servono più perché ho già generato i file necessari, che puoi trovare nella mia cartella /ubuntu (non dovrebbero esserci problemi di permessi). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import SGD, Adam, Adadelta, RMSprop\n",
    "from tensorflow.keras.layers import Conv1D, Dense, MaxPooling1D, Flatten, Dropout\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, LSTM, SimpleRNN, GRU\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intr_coords = pd.read_csv('Akey_intr_coords.bed', header = None, sep = \"\\t\")\n",
    "intr_coords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intr_lengths = intr_coords.iloc[:, 2]-intr_coords.iloc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats.describe(intr_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!zcat hg19.fa.gz | bgzip -c > hg19.fa.bgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "with open('hg19_intr_regions.fa', 'a') as fp:\n",
    "    for i in range(intr_coords.shape[0]):\n",
    "        coord = str(str(intr_coords.iloc[i, 0]) + ':' \n",
    "                    + str(intr_coords.iloc[i, 1]) + '-' + str(intr_coords.iloc[i, 2]))\n",
    "        subprocess.run(['samtools', 'faidx', 'hg19.fa.bgz', str(coord)], stdout = fp)\n",
    "        a = a + 1\n",
    "        if a%10000 == 0:\n",
    "            print('Finished ' + str(a) + ' Neanderthal introgressed haplotypes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"chr_sizes = pd.read_csv(\"hg19.fa.bgz.fai\", header = None, sep = \"\\t\")\n",
    "chr_sizes = chr_sizes.drop([2, 3, 4], axis = 1)\n",
    "chr_sizes.head()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"chr_list = []\n",
    "start_list = []\n",
    "end_list = []\n",
    "intr_lengths = list(intr_coords.iloc[:, 2] - intr_coords.iloc[:, 1])\n",
    "a = 0\n",
    "for i in range(intr_coords.shape[0]):\n",
    "    chr_df = intr_coords[intr_coords[0].isin([intr_coords.iloc[i,0]])]\n",
    "    overlap = True\n",
    "    while overlap == True:\n",
    "        reg_start = np.random.randint(1, int(chr_sizes[chr_sizes[0] == intr_coords.iloc[i,0]].iloc[:,1]))\n",
    "        reg_end = reg_start + intr_lengths[i]\n",
    "        for j in range(chr_df.shape[0]):\n",
    "            b1 = chr_df.iloc[j,1]\n",
    "            b2 = chr_df.iloc[j,2]\n",
    "            if (reg_start > b1 and reg_start < b2) or (reg_end > b1 and reg_end < b2) or \\\n",
    "            (b1 > reg_start and b1 < reg_end) or (b2 > reg_start and b2 < reg_end):\n",
    "                overlap = True\n",
    "                break\n",
    "            else:\n",
    "                overlap = False\n",
    "    chr_list.append(intr_coords.iloc[i,0])\n",
    "    start_list.append(reg_start)\n",
    "    end_list.append(reg_end)\n",
    "    a = a + 1\n",
    "    if a%10000 == 0:\n",
    "            print('Finished ' + str(a) + ' Neanderthal introgressed haplotypes')\n",
    "depl_coords = pd.DataFrame({'0': chr_list, '1': start_list, '2': end_list})\n",
    "depl_coords.to_csv(\"Akey_depl_coords.bed\", index = False, header = False, sep = \"\\t\")\n",
    "depl_coords.head()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depl_coords = pd.read_csv(\"Akey_depl_coords.bed\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bedtools intersect -a Akey_intr_coords.bed -b Akey_depl_coords.bed | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "with open('hg19_depl_regions.fa', 'a') as fp:\n",
    "    for i in range(depl_coords.shape[0]):\n",
    "        coord = str(str(depl_coords.iloc[i, 0]) + ':' \n",
    "                    + str(depl_coords.iloc[i, 1]) + '-' + str(depl_coords.iloc[i, 2]))\n",
    "        subprocess.run(['samtools', 'faidx', 'hg19.fa.bgz', str(coord)], stdout = fp)\n",
    "        a = a + 1\n",
    "        if a%10000 == 0:\n",
    "            print('Finished ' + str(a) + ' Neanderthal ancestry depleted regions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep -c N hg19_intr_regions.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep -c N hg19_depl_regions.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intr_file = 'hg19_intr_regions.fa'\n",
    "depl_file = 'hg19_depl_regions.fa'\n",
    "a = 0\n",
    "i = 0\n",
    "with open('hg19_intr_clean.fa', 'a') as intr_out, open('hg19_depl_clean.fa', 'a') as depl_out:\n",
    "    for intr, depl in zip(SeqIO.parse(intr_file, 'fasta'), SeqIO.parse(depl_file, 'fasta')):\n",
    "        upper_intr = intr.seq.upper()\n",
    "        upper_depl = depl.seq.upper()\n",
    "        a = a + 1\n",
    "        if a%10000 == 0:\n",
    "            print('Finished ' + str(a) + ' entries')\n",
    "        if 'N' not in str(upper_intr) and 'N' not in str(upper_depl):\n",
    "            intr.seq = upper_intr\n",
    "            SeqIO.write(intr, intr_out, 'fasta')\n",
    "            depl.seq = upper_depl\n",
    "            SeqIO.write(depl, depl_out, 'fasta')\n",
    "            i = i + 1\n",
    "        else:\n",
    "            continue\n",
    "print('We have processed ' + str(a) + ' entries and written ' + str(i) + ' entries to two fasta-files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep -c N hg19_intr_regions.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep -c N hg19_depl_regions.fa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START HERE\n",
    "\n",
    "Qui il codice commentato è perché volevo fare un po' di prove, quindi può essere cambiato a piacimento\n",
    "\n",
    "Attenzione perché la rete così com'è ci mette tanto ad allenarsi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 20000 entries\n",
      "Finished 40000 entries\n",
      "Finished 60000 entries\n"
     ]
    }
   ],
   "source": [
    "intr_file = '../data/hg19_intr_clean.fa'\n",
    "depl_file = '../data/hg19_depl_clean.fa'\n",
    "\n",
    "\"\"\"a = 0\n",
    "intr_seqs = []\n",
    "depl_seqs = []\n",
    "for intr, depl in zip(SeqIO.parse(intr_file, 'fasta'), SeqIO.parse(depl_file, 'fasta')):\n",
    "    cut = 1000\n",
    "    if len(str(intr.seq)) < cut or len(str(depl.seq)) < cut:\n",
    "        continue\n",
    "    s_intr = str(intr.seq)[0:cut]\n",
    "    s_depl = str(depl.seq)[0:cut]\n",
    "    if s_intr.count('A')>0 and s_intr.count('C')>0 and s_intr.count('G')>0 and s_intr.count('T')>0 and \\\n",
    "    s_depl.count('A')>0 and s_depl.count('C')>0 and s_depl.count('G')>0 and s_depl.count('T')>0:\n",
    "        intr_seqs.append(s_intr)\n",
    "        depl_seqs.append(s_depl)\n",
    "    a = a + 1\n",
    "    if a%10000 == 0:\n",
    "        print('Finished ' + str(a) + ' entries')\"\"\"\n",
    "\n",
    "e = 0\n",
    "intr_seqs = []\n",
    "depl_seqs = []\n",
    "for intr, depl in zip(SeqIO.parse(intr_file, 'fasta'), SeqIO.parse(depl_file, 'fasta')):\n",
    "    \n",
    "    #cutoff = 200\n",
    "    #my_intr_seq = str(intr.seq)[0:cutoff]\n",
    "    #my_depl_seq = str(depl.seq)[0:cutoff]\n",
    "    #intr_seqs.append(my_intr_seq)\n",
    "    #depl_seqs.append(my_depl_seq)\n",
    "    \n",
    "    step = 200; jump = 1; a = 0; b = step; n_jumps = 5\n",
    "    for j in range(n_jumps):\n",
    "        s_intr = str(intr.seq)[a:b]\n",
    "        s_depl = str(depl.seq)[a:b]\n",
    "        intr_seqs.append(s_intr)\n",
    "        depl_seqs.append(s_depl)\n",
    "        a = a + jump\n",
    "        b = a + step\n",
    "    \n",
    "    e = e + 1\n",
    "    if e%20000 == 0:\n",
    "        print('Finished ' + str(e) + ' entries')\n",
    "        \n",
    "def getKmers(sequence, size):\n",
    "    return [sequence[x:x+size].upper() for x in range(len(sequence) - size + 1)]\n",
    "\n",
    "kmer = 10\n",
    "intr_texts = [' '.join(getKmers(i, kmer)) for i in intr_seqs]\n",
    "depl_texts = [' '.join(getKmers(i, kmer)) for i in depl_seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "733640"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"sequences = intr_seqs + depl_seqs\n",
    "len(sequences)\"\"\"\n",
    "\n",
    "merge_texts = intr_texts + depl_texts\n",
    "len(merge_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "733640\n"
     ]
    }
   ],
   "source": [
    "\"\"\"labels = list(np.ones(len(intr_seqs))) + list(np.zeros(len(depl_seqs)))\n",
    "len(labels)\"\"\"\n",
    "\n",
    "labels = list(np.ones(len(intr_texts))) + list(np.zeros(len(depl_texts)))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d3498fb4e1e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#X = tokenizer.texts_to_matrix(merge_texts, mode = 'freq')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras_preprocessing/text.py\u001b[0m in \u001b[0;36mfit_on_texts\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;31m# note that index 0 is reserved, never assigned to an existing word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         self.word_index = dict(\n\u001b[0;32m--> 244\u001b[0;31m             list(zip(sorted_voc, list(range(1, len(sorted_voc) + 1)))))\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#cv = CountVectorizer()\n",
    "#X = cv.fit_transform(merge_texts)\n",
    "\n",
    "#tfidf_transformer = TfidfTransformer()\n",
    "#X = tfidf_transformer.fit_transform(X)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(merge_texts)\n",
    "#X = tokenizer.texts_to_matrix(merge_texts, mode = 'freq')\n",
    "\n",
    "encoded_docs = tokenizer.texts_to_sequences(merge_texts)\n",
    "max_length = max([len(s.split()) for s in merge_texts])\n",
    "X = pad_sequences(encoded_docs, maxlen = max_length, padding = 'post')\n",
    "\n",
    "print(X)\n",
    "print('\\n')\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = X_train[:10000]\n",
    "X_test1 = X_test[:1000]\n",
    "y_train1 = y_train[:10000]\n",
    "y_test1 = y_test[:1000]\n",
    "print(X_train1.shape)\n",
    "print(X_test1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(s.split()) for s in merge_texts])\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"model = Sequential()\n",
    "#model.add(Dense(3000, input_shape = (X.shape[1],), activation = 'sigmoid'))\n",
    "model.add(Embedding(vocab_size, 32, input_length = max_length, dropout = 0.2))\n",
    "#model.add(Conv1D(filters = 16, kernel_size = 5, padding = 'same', activation = 'relu'))\n",
    "#model.add(MaxPooling1D(pool_size = 2))\n",
    "#model.add(LSTM(10)) #dropout = 0.2, recurrent_dropout = 0.2\n",
    "#model.add(GRU(10))\n",
    "model.add(SimpleRNN(10, dropout = 0.2, recurrent_dropout = 0.2))\n",
    "#model.add(Flatten())\n",
    "#model.add(Dense(10, activation = 'sigmoid'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "epochs = 5\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'rmsprop', metrics = ['accuracy'])\n",
    "#model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#model.compile(loss = 'binary_crossentropy', optimizer = 'SGD', metrics = ['accuracy'])\n",
    "#model.compile(loss = 'binary_crossentropy', optimizer = RMSprop(lr = 0.0001), metrics = ['accuracy'])\n",
    "checkpoint = ModelCheckpoint(\"weights.best.hdf5\", monitor = 'val_acc', verbose = 1, \n",
    "                             save_best_only = True, mode = 'max')\n",
    "print(model.summary())\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, LSTM, SimpleRNN, GRU, Bidirectional\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10)) #dropout = 0.2 #input_length = max_length\n",
    "#model.add(Conv1D(filters = 16, kernel_size = 5, padding = 'same', activation = 'relu'))\n",
    "#model.add(MaxPooling1D(pool_size = 2))\n",
    "#model.add(LSTM(10)) #dropout = 0.2, recurrent_dropout = 0.2\n",
    "model.add(Bidirectional(LSTM(10))) #dropout = 0.2, recurrent_dropout = 0.2\n",
    "#model.add(Bidirectional(SimpleRNN(10)))\n",
    "#model.add(GRU(10))\n",
    "#model.add(SimpleRNN(10, dropout = 0.2, recurrent_dropout = 0.2))\n",
    "#model.add(Flatten())\n",
    "model.add(Dense(10, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "epochs = 5\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'rmsprop', metrics = ['accuracy'])\n",
    "#model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#model.compile(loss = 'binary_crossentropy', optimizer = 'SGD', metrics = ['accuracy'])\n",
    "#model.compile(loss = 'binary_crossentropy', optimizer = RMSprop(lr = 0.0001), metrics = ['accuracy'])\n",
    "checkpoint = ModelCheckpoint(\"weights.best.hdf5\", monitor = 'val_acc', verbose = 1, \n",
    "                             save_best_only = True, mode = 'max')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train1, y_train1, \n",
    "                    epochs = epochs, verbose = 1, validation_split = 0.2, batch_size = 32, shuffle = True, \n",
    "                    callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "\n",
    "predicted_labels = model.predict(X_test1)\n",
    "cm = confusion_matrix(y_test1, [np.round(i[0]) for i in predicted_labels])\n",
    "print('Confusion matrix:\\n',cm)\n",
    "\n",
    "cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]\n",
    "\n",
    "plt.imshow(cm, cmap = plt.cm.Blues)\n",
    "plt.title('Normalized confusion matrix', fontsize = 20)\n",
    "plt.colorbar()\n",
    "plt.xlabel('True label', fontsize = 20)\n",
    "plt.ylabel('Predicted label', fontsize = 20)\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], '.2f'),\n",
    "             horizontalalignment = 'center', verticalalignment = 'center', fontsize = 20,\n",
    "             color='white' if cm[i, j] > 0.5 else 'black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test1, y_test1, verbose = 0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
